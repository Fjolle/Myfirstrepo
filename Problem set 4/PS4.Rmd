---
title: "4th Problem Set"
author: "Fjolle Gjonbalaj"
date: "07/05/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#loading the data and the libraries
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(kableExtra)
library(foreach)
library(data.table)
library(tidyverse)
library(arules)
library(arulesViz)


library(tidyverse)
library(tm)
library(gamlr)
library(glmnet)
library(SnowballC)
library(slam)
library(proxy)
```

# Question 1: Clustering and PCA

This problem compares unsupervised learning methods like Principal Component Analysis and K-means clustering to find out whether or not they can successfully distinguish the red wine from the white wine, as well as sorting the higher from the lower quality of wine based on the dataset with 11 chemical properties (or suitable transformations thereof) of 6500 different bottles of vinho verde wine from northern Portugal. 

# Unsupervised learning methods to distinguish red wine from white wine

```{r , echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
wine1=qplot(x = color, y = fixed.acidity, geom = "boxplot", data = wine)
wine2=qplot(x = color, y = volatile.acidity, geom = "boxplot", data = wine)
wine3=qplot(x = color, y = citric.acid, geom = "boxplot", data = wine)
wine4=qplot(x = color, y = residual.sugar, geom = "boxplot", data = wine)
wine5=qplot(x = color, y = chlorides, geom = "boxplot", data = wine)
wine6=qplot(x = color, y = free.sulfur.dioxide, geom = "boxplot", data = wine)
wine7=qplot(x = color, y = total.sulfur.dioxide, geom = "boxplot", data = wine)
wine8=qplot(x = color, y = density, geom = "boxplot", data = wine)
wine9=qplot(x = color, y = pH, geom = "boxplot", data = wine)
wine10=qplot(x = color, y = sulphates, geom = "boxplot", data = wine)
wine11=qplot(x = color, y = alcohol, geom = "boxplot", data = wine)
wine12=qplot(x = color, y = quality, geom = "boxplot", data = wine)
grid.arrange(wine1, wine2, wine3, wine4, wine5, wine6, wine7, wine8, wine9,
             wine10, wine11, wine12, nrow = 4, 
             top ="Chemical properties of wine based on the color of wine")
```

The boxplots show that red and white wine are distinct from one-another when it comes to the chemical components such as total sulfur dioxide, residual sugar, volatile acidity and fixed acidity. I use these features in order to analyze the performance of the selected methods.In the figure with boxplots the wine colors are not very different in the other chemical features. As such, a visual graphic is chosen to show  the performance of K-means clustering on the selected chemical features. 

# K-means clustering

```{r , echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
center = wine[,(1:11)]
center = scale(center, center=TRUE, scale=TRUE)
mu = attr(center,"scaled:center")
sigma = attr(center,"scaled:scale")
set.seed(1)
ClusterWine = kmeans(center, 2, nstart=25)
qplot(fixed.acidity, residual.sugar , data=wine, color=factor(ClusterWine$cluster))
qplot(fixed.acidity, residual.sugar , data=wine, color=factor(wine$color))
qplot(total.sulfur.dioxide, volatile.acidity, data=wine, color=factor(ClusterWine$cluster))
qplot(total.sulfur.dioxide, volatile.acidity, data=wine, color=factor(wine$color))
```


The above figures are a visual representation of the K-means clustering, and how the latter is able to successfully differentiate the colors of wine with respect to the chemical properties.

# Prinicipal Components Analysis (PCA) by wine colors

Principal Components Analysis performs in a similar manner to distinguish wine by the color. The table below displays that the first five principal components explain the cumulative variation in the data. 

```{r , echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
Wine_PCA = prcomp(center, scale=TRUE)
sum_Wine_PCA=data.frame(summary(Wine_PCA)$importance)
sum_Wine_PCA=sum_Wine_PCA[ , 1:11]
kable(sum_Wine_PCA, caption="PCA by wine colors", format_caption = c("italic", "underline"), booktabs=TRUE) %>%
  kable_styling(bootstrap_options = "basic", full_width = T)
plot(Wine_PCA, main = "Variance according to the principal components", xlab='Principal Components', color="lightblue")
loadings = Wine_PCA$rotation
```



```{r , echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
rotation_Wine_PCA=data.frame(round(Wine_PCA$rotation[,1:5],2))
kable(rotation_Wine_PCA, caption="Loadings of the first five Principal Components", format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "basic", full_width = F)
```

The loadings for the first five principal components are the linear combinations of the original chemical properties. For instance, free sulfur dioxide and total sulfur dioxide are strongly positively correlated with the first principal component, whereas volatile acidity and sulfate are strongly and negatively correlated with the first principal component.


```{r , echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
scores = Wine_PCA$x
qplot(scores[,1], scores[,2], color=wine$color, xlab='Component 1', ylab='Component 2', main = "PCA for wine colors based on the first two principal components")
```

The plot above shows that PCA can also perform the clustering based on the type of wine. It can be seen that white wine generally clusters around positive values in the dimension of the first principal component(although not always), whereas red wine generally clusters around the negative values. The K-means clustering on the first five principal components can be used to check whether or not we can improve the partitioning of wine color based on K-means. This can be successfully performed by augmenting the K-means clustering. 

```{r ,echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
WinePCAclust = kmeans(scores[,1:5], 2, nstart=20)
w3.1 <- qplot(fixed.acidity, residual.sugar , data=wine, color=factor(WinePCAclust$cluster))
w3.2 <- qplot(fixed.acidity, residual.sugar , data=wine, color=factor(wine$color))
grid.arrange(w3.1, w3.2, nrow = 2, top = "K-means clustering PCA for wine color
based on total sulfur dioxide and volatile acidity.")  
```

```{r , echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
With_Bet = rbind("Within-cluster SS of Basic clustering " = ClusterWine$withbet, 
                 "Within-cluster SS of Clustering based PCA " = WinePCAclust$withbet, 
                 "Between-cluster SS of Basic clustering " = ClusterWine$between, 
                 "Between-cluster SS of Clustering based PCA " = WinePCAclust$between)
kable(With_Bet, caption="In-sample fit of two clusters")
```

It can be shown that both PCA and K-means can successfully make the difference when it comes to the color of the wine using unsupervised learning on chemical properties. However, by first reducing the dimensions of the features by PCA and then implementing K-means would result in the colors being even better differentiated. This results in reduced within-cluster sum of squares. 

# Unsupervised learning on the quality of wine

```{r , echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
ggplot(wine)+
  geom_bar(aes(x = quality, fill = color, binwidth = 1))+
             labs(title = " Wine quality")
```

The above figure shows that there are 7 different qualities of wines that the data contains. We can further define the higher-quality wine as wine that takes a score higher than 7;The medium-quality wine takes a value somewhere between 4 and 7; whereas the lower quality wine takes a score of lower than 4. We can then further explore which chemical components of wine quality are the most different among each other.

```{r , echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
wine$qualityindicator <- ifelse(wine$quality <= 4, 'low', ifelse(wine$quality <= 7, 'medium', 'high'))
wine_quality1 <- qplot(x = qualityindicator, y = citric.acid, data = wine, geom = "boxplot")
wine_quality2 <- qplot(x = qualityindicator, y = volatile.acidity, data = wine, geom = "boxplot")
wine_quality3 <- qplot(x = qualityindicator, y = citric.acid, data = wine, geom = "boxplot")
wine_quality4 <- qplot(x = qualityindicator, y = residual.sugar, data = wine, geom = "boxplot")
wine_quality5 <- qplot(x = qualityindicator, y = chlorides, data = wine, geom = "boxplot")
wine_quality6 <- qplot(x = qualityindicator, y = free.sulfur.dioxide, data = wine, geom = "boxplot")
wine_quality7 <- qplot(x = qualityindicator, y = total.sulfur.dioxide, data = wine, geom = "boxplot")
wine_quality8 <- qplot(x = qualityindicator, y = density, data = wine, geom = "boxplot")
wine_quality9 <- qplot(x = qualityindicator, y = pH, data = wine, geom = "boxplot")
wine_quality10 <- qplot(x = qualityindicator, y = sulphates, data = wine, geom = "boxplot")
wine_quality11 <- qplot(x = qualityindicator, y = alcohol, data = wine, geom = "boxplot")
grid.arrange(wine_quality1, wine_quality2, wine_quality3, wine_quality4, wine_quality5, wine_quality6, wine_quality7, wine_quality8, wine_quality9, wine_quality10, wine_quality11, nrow = 3, top = "Chemical properties among three wine quality levels")
```

It is seen in the boxplots above that higher and lower quality wine are the ones that vary the most based on the chemical component.

# K-means clustering on the quality levels of wine

```{r, echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
wine$qualityindicator <- ifelse(wine$quality <= 4, 'low', ifelse(wine$quality <= 7, 'medium', 'high'))
set.seed(5)
c = kmeans(center, 3, nstart=20)
wine_quality1.1 <- qplot(alcohol, volatile.acidity,  data=wine, color=factor(c$cluster), alpha=I(0.5))
wine_quality1.2 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.5)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high", "medium")))
wine_quality1.8 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.5)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
wine_quality1.5 <- ggplot(wine, aes(free.sulfur.dioxide, chlorides))+
  geom_point(aes(color=factor(c$cluster),alpha = I(0.3)))
wine_quality1.6 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.5)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high","medium")))
wine_quality1.10 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.5)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
grid.arrange(wine_quality1.1, wine_quality1.5, wine_quality1.2, wine_quality1.6, wine_quality1.8, wine_quality1.10, nrow = 3, top = "K-means clustering for wine quality.")
```

# Prinicipal Components Analysis (PCA) on higher and lower quality wine

```{r , echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
wine$qualityindicator <- ifelse(wine$quality <= 4, 'low', ifelse(wine$quality <= 7, 'medium', 'high'))
pca_wine_quality = prcomp(center, scale=TRUE)
scores2 = pca_wine_quality$x
qplot(scores2[,1], scores2[,2], color=wine$qualityindicator, xlab='1st Component', ylab='2nd Component', main = "PCA for wine quality on principal components", alpha=I(0.5))
```

It is observed from the plot above that higher quality wine generally cluster to the right of the principal components, while the lower quality wine generally cluster to the left of them.

```{r ,echo = FALSE, warning=FALSE}
wine <- read.csv("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/wine.csv")
wine$qualityindicator <- ifelse(wine$quality <= 4, 'low', ifelse(wine$quality <= 7, 'medium', 'high'))
set.seed(5)
pc = kmeans(scores2[,1:5], 3, nstart=20)
wine_quality1.11 <- qplot(alcohol, volatile.acidity,  data=wine, color=factor(pc$cluster), alpha=I(0.5))
wine_quality1.12 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.5)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high", "medium")))
wine_quality1.18 <- ggplot(wine, aes(x=alcohol, y = volatile.acidity, color=qualityindicator, alpha = I(0.5)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
wine_quality1.15 <- ggplot(wine, aes(free.sulfur.dioxide, chlorides))+
  geom_point(aes(color=factor(pc$cluster),alpha = I(0.5)))
wine_quality1.16 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.5)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high","medium")))
wine_quality1.110 <- ggplot(wine, aes(x=free.sulfur.dioxide, y = chlorides, color=qualityindicator, alpha = I(0.5)))+
  geom_point(data = subset(wine, qualityindicator %in% c("low","high")))
grid.arrange(wine_quality1.11, wine_quality1.15, wine_quality1.12, wine_quality1.16, wine_quality1.18, wine_quality1.110, nrow = 3, top = "K-means clustering from PCA for wine quality")
```

K-means clustering on the first five principal components is used to check whether or not partitioning of wine quality according to K-means can be improved upon. The figure above shows how k-means clustering partitions the data into three clusters in the dimensions of volatile acidity and alcohol. The middle left plot shows how the three wine quality levels are different in these dimensions. It can be difficult to observe the difference between the lower and higher quality wine due to a large number of middle quality wine that overlap.
To conclude, neither K-means clustering nor PCA successfully differentiate the higher from the lower quality wines using only unsupervised learning contained on chemical properties of wine. Nevertheless, reducing the dimensions of the features by PCA and then applying K-means clustering could make the successful distinction on the quality levels of wine which could be observed from the reduced within-cluster sum of squares. Hence, K-means clustering together with PCA on the chemical components of wine work more successfully to differentiate between wine color than when we compare them based on the wine quality. 


# Question 2:  Market segmentation

# K-means

```{r , warning=FALSE, echo=FALSE}
social_marketing=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(social_marketing, social_marketing$spam == 0 & social_marketing$adult == 0)
soc_market = socialmarketing[ , c(-1, -36, -37)]
soc_market=soc_market/rowSums(soc_market)
soc_markets=scale(soc_market, center = TRUE, scale = TRUE)
Ns = nrow(soc_markets)
k_grid = seq(1, 10, by=1)
grids = foreach(k = k_grid, .combine='c') %do% {
  cluster_ks = kmeans(soc_markets, k, nstart=25)
  With = cluster_ks$within
  Bet = cluster_ks$between
  CH = (Bet/With)*((Ns-k)/(k-1))
}
plot(k_grid,elbow.obj=NULL,xlab="k",
ylab="Explained_Variance",type="b",pch=20,col.abline="red",
lty.abline=3,if.plot.new=TRUE,print.info=TRUE,
mar=c(4,5,3,3),omi=c(0.75,0,0,0))
```

The figure above shows the elbow plot. It is not clear from the graph what the appropriate elbow is.

# Result from K-means

```{r ,  warning=FALSE, echo=FALSE}
social_marketing=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(social_marketing, social_marketing$spam == 0 & social_marketing$adult == 0)
soc_market = socialmarketing[ , c(-1, -36, -37)]
soc_market=soc_market/rowSums(soc_market)
soc_markets=scale(soc_market, center = TRUE, scale = TRUE)
set.seed(1)
clust_k= kmeans(soc_markets, 10, nstart=20)
result=data.frame(clust_k$cluster)
resultCombinedData=cbind(soc_market, result$clust_k.cluster)
table_kmeans=data.frame("Cluster"=character(11))
for (i in 1:10) {
  Cluster = filter(resultCombinedData, result$clust_k.cluster == i)
  ClusterMeans = as.data.table(colMeans(Cluster), keep.rownames=TRUE)
  colnames(ClusterMeans) <- c("Categories", "Cluster Means")
  ClusterMeans = ClusterMeans[order(-`Cluster Means`)]
  ClusterMeans = ClusterMeans[1:10,1]
  c=data.frame(nrow(Cluster))
  ClusterMeans=rbind(ClusterMeans,c, use.names=FALSE)
  table_kmeans=cbind(table_kmeans, ClusterMeans)
}
table_kmeans=table_kmeans[-1, -1]
table_kmeans=transpose(table_kmeans)
table_kmeans=table_kmeans[order(table_kmeans[, ncol(table_kmeans)], decreasing = TRUE),]
rownames(table_kmeans)=c("Cluster a", "Cluster b", "Cluster c", "Cluster d", "Cluster e", "Cluster f", "Cluster g", "Cluster h", "Cluster i", "Cluster j")
colnames(table_kmeans)=c("Category a", "Category b", "Category c", "Category d", "Category e",
                         "Consumer_no.")
kable(table_kmeans, caption ="Clusters of K-means" ,format_caption = c("bold", "underline")) %>%
  kable_styling(bootstrap_options = "basic", full_width = F)
```

K-means gave stable clusters of consumers. For each cluster, the sum of frequencies for each category over all consumers in that cluster is calculated. After ordering the categories by sums of frequencies, the first five categories with higher sums of frequencies are picked out to represent that specific cluster. 

# Principal Component Analysis and Hierarchical Clustering

```{r,warning=FALSE, echo=FALSE}
social_marketing=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(social_marketing, social_marketing$spam == 0 & social_marketing$adult == 0)
soc_market = socialmarketing[ , c(-1, -36, -37)]
soc_market=soc_market/rowSums(soc_market)
soc_markets=scale(soc_market, center = TRUE, scale = TRUE)

pca1=prcomp(soc_market)
pve1=100*pca1$sdev^2/sum(pca1$sdev^2)

pca2=prcomp(soc_market, scale=TRUE)
pve2=100*pca2$sdev^2/sum(pca2$sdev^2)

soc_market2 = socialmarketing[ , c(-1, -36, -37)]
pca3=prcomp(soc_market2, scale=TRUE)
pve3=100*pca3$sdev^2/sum(pca3$sdev^2)

PC=c(1:34)
pvecombine=cbind.data.frame(PC, pve1, pve2, pve3)
ggplot(data=pvecombine)+
  geom_line(aes(x=PC,y=pve1), col="green2")+geom_point(aes(x=PC, y=pve1), col="green2", shape=2, size=2)+
  geom_line(aes(x=PC,y=pve2), col="lightblue")+geom_point(aes(x=PC, y=pve2), col="lightblue", shape=2, size=2)+
  geom_line(aes(x=PC,y=pve3), col="pink")+geom_point(aes(x=PC, y=pve3), col="pink", shape=2, size=2)+
  labs(y="PVE", x="Principal Component")+
  theme_bw()+ labs(title = " PVE of PCA")+
  theme(plot.title = element_text(hjust = 0.5))
```

A segmentation by PCA is performed by first scaling the data on PCA using three different scales. The proportion of variance of each principal component is calculated after performing PCA on each different scale. The figure above shows how PVE of each scale varies with each principal component, where the green line corresponds to the first scale, the light blue line corresponds to the second scale, and the pink one corresponds to the third scale. Based on the location of elbow in the figure the first ten principal components were chosen,which is a representative number of the data. 

```{r ,  warning=FALSE, echo=FALSE}
social_marketing=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(social_marketing, social_marketing$spam == 0 & social_marketing$adult == 0)
soc_market = socialmarketing[ , c(-1, -36, -37)]
soc_market=soc_market/rowSums(soc_market)
soc_markets=scale(soc_market, center = TRUE, scale = TRUE)

pca1=prcomp(soc_market)
pve1=100*pca1$sdev^2/sum(pca1$sdev^2)

sum_pca1=data.frame(summary(pca1)$importance)
sum_pca1=sum_pca1[ , 1:11]
kable(sum_pca1, caption ="Variations in Principal Components", format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "basic", full_width = F)
```


Each component is used to differentiate the consumers. 

```{r,  warning=FALSE, echo=FALSE}
social_marketing=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(social_marketing, social_marketing$spam == 0 & social_marketing$adult == 0)
soc_market = socialmarketing[ , c(-1, -36, -37)]
soc_market=soc_market/rowSums(soc_market)
soc_markets=scale(soc_market, center = TRUE, scale = TRUE)

pca1=prcomp(soc_market)
pca1_loading=pca1$rotation[ , c(1:11)]

pc=data.frame(rep(1:34))
for (i in c(1:11)){
  sort=data.frame(sort(pca1_loading[,i], decreasing = TRUE))
  rownames=data.frame(row.names(sort))
  c=cbind(rownames, sort[,1])
  pc=cbind(pc,c)
}
pc=pc[,-1]
colnames(pc)=c("Categories", "PC1", "Categories", "PC2", "Categories", "PC3", "Categories", "PC4", "Categories", "PC5",
               "Categories", "PC6", "Categories", "PC7", "Categories", "PC8", "Categories", "PC9", "Categories", "PC10")

tablePCA=pc[1:5,-c(seq(1, 10, by=1))]

colnames(tablePCA)=c("Pri_Com 1", "Pri_Com 2", "Pri_Com 3", "Pri_Com 4", "Pri_Com 5","Pri_Com 6","Pri_Com 7", "Pri_Com 8", "Pri_Com 9", "Pri_Com 10")

rownames(tablePCA)=c("Category 1", "Category 2", "Category 3", "Category 4", "Category 5")
kable(tablePCA,  caption ="Principal Components", format_caption = c("italic", "underline")) %>%
  kable_styling(bootstrap_options = "basic", full_width = T)
```

Problem with the above result is that we are not certain who the consumers are and which ones have an interest in which cluster.To solve this problem, the scores of PCA can be used to construct a distance matrix, which measures the distance between each pair of consumers in the data. By Hierarchical Clustering and cutting we are able to obtain 10 clusters of consumers, in accordance with the value of K in K-means. The Hierarchical Clustering with K-means finds out a hierarchical structure of categories based on the proximity matrix of correlations between each pair of categories. Below you find the Dendogram of Hierarchical Clustering.

```{r, warning=FALSE, echo=FALSE}
social_marketing=read.csv('https://raw.githubusercontent.com/jgscott/ECO395M/master/data/social_marketing.csv')
socialmarketing=filter(social_marketing, social_marketing$spam == 0 & social_marketing$adult == 0)
soc_market = socialmarketing[ , c(-1, -36, -37)]
soc_market=soc_market/rowSums(soc_market)
soc_markets=scale(soc_market, center = TRUE, scale = TRUE)

soc_markets_corr=cor(soc_markets)
d_soc_marketscorr=dist(soc_markets_corr)

correl_hc1=hclust(d_soc_marketscorr, method = "complete")
plot(correl_hc1, cex=1, xlab="", sub="", main="Dendrogram of Hierarchical Clustering")
```

To conclude, there exists one to one mapping between ten clusters of categories and ten clusters of consumers. Moreover, the  distribution of the numbers of consumers over ten clusters is extremely close. By comparing the number of consumers we can link the clusters in any of the three tables. The stable clustering structures across different approaches proves that our ten clusters are very robust based on our data. The largest cluster of the customers of NurientH20 is the one focusing on health and workout. The second largest is the online socializing and shopping.





# QUESTION 3: Association rules for grocery purchases

`
```{r, warning=FALSE, echo=FALSE}
library(tidyverse)
library(arules) 
library(arulesViz)
groceries = read.delim("https://raw.githubusercontent.com/jgscott/ECO395M/master/data/groceries.txt", header = FALSE)
groceries = as.data.frame(groceries)
```

Among the associated rules computed, I narrow down the rules into subsets according to confidence, lift and support. 

```{r , warning=FALSE, echo=FALSE}
lists <- strsplit(groceries$V1, split = ",")
all_lists <- lapply(lists, unique)
groceries_trans = as(all_lists, "transactions")
grocery_rules = apriori(groceries_trans, 
	parameter=list(support=.001, confidence=.2, maxlen=1))
```

```{r, warning=FALSE, echo=FALSE}                         
arules::inspect(grocery_rules)
arules::inspect(subset(grocery_rules, lift > 1))
arules::inspect(subset(grocery_rules, confidence > 0.23))
arules::inspect(subset(grocery_rules, lift > 1 & confidence > 0.04))
plot(grocery_rules)
grocrules_2 = apriori(groceries_trans, 
parameter=list(support=0.002, confidence=0.8, minlen=1))
arules::inspect(grocrules_2)
plot(grocrules_2)
plot(head(grocrules_2, 5, by='lift'), method='graph')
```


The association can be vizualized through a network graph. The larger the label, the more frequent the transaction. "Whole milk," and "other vegetables" seem to tie together most transactions. 

A strong association between “whole milk” and “other vegetables,” is observed. This is perhaps due to  them generally being considered staple goods. 

A low level of support chosen was driven by the fact that there weren’t too many different grocery items within the dataset.

In general the rules generated were based on the intuition that, say, that if a customer buys root vegetables he/she is also more likely to buy other vegetables. When it comes to the intuition, lift might be the most informative measure. This is because it measures the conditional probability of purchasing item set A given that you already purchased item set B. Therefore, while lift takes into consideration statistical dependence, confidence and support do not. 


The chosen confidence is 80%, support rate is 2% and the minimum lift is 1. This enables for the realization of any association between different grocery baskets. 




# QUESTION 4: Author attribution


```{r echo=FALSE, warning=FALSE, include=FALSE}
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
train_direct = Sys.glob('C:/Users/Fjolla/Documents/GitHub/ECO395M/data/ReutersC50/C50train/*')
train_direct = train_direct[c(1:50)]
file_list = NULL
labels_train = NULL
for(author in train_direct) {
	author_name = substring(author, first=1)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list = append(file_list, files_to_add)
	labels_train = append(labels_train, rep(author_name, length(files_to_add)))
}
train_direct
corpus_train = Corpus(DirSource(train_direct)) 
corpus_train = corpus_train %>% tm_map(., content_transformer(tolower)) %>% 
        tm_map(., content_transformer(removeNumbers)) %>% 
				tm_map(., content_transformer(removeNumbers)) %>% 
				tm_map(., content_transformer(removePunctuation)) %>%
				tm_map(., content_transformer(stripWhitespace)) %>%
				tm_map(., content_transformer(removeWords), stopwords("SMART"))

```


```{r}
TermMatrix_train = DocumentTermMatrix(corpus_train)
TermMatrix_train 
TermMatrix_train2 = removeSparseTerms(TermMatrix_train, 0.95)
TermMatrix_train2
DF_train <- data.frame(as.matrix(TermMatrix_train2), stringsAsFactors=FALSE)
labels_train = append(labels_train, rep(author_name, length(files_to_add)))
#Cleaning label names
Author_name = labels_train %>%
	{ strsplit(., '/', fixed=TRUE) } %>%
	{ lapply(., tail, n=2) } %>%
	{ lapply(., paste0, collapse = '') } %>%
	unlist
Author_name = as.data.frame(Author_name)
Author_name = gsub("C([0-9]+)train", "\\1", Author_name$Author_name)
Author_name = gsub("([0-9]+)", "", Author_name)
Author_name = as.data.frame(Author_name)
```

```{r echo=FALSE, warning=FALSE}
test_direct = Sys.glob('C:/Users/Fjolla/Documents/GitHub/ECO395M/data/ReutersC50/C50test/*')
test_direct = test_direct[c(1: 50)]
file_list = NULL
labels_test = NULL
for(author in test_direct) {
	author_name = substring(author, first=1)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list = append(file_list, files_to_add)
	labels_test = append(labels_test, rep(author_name, length(files_to_add)))
}
test_direct
corpus_test = Corpus(DirSource(test_direct)) 
corpus_test = corpus_test %>% tm_map(., content_transformer(tolower)) %>% 
				tm_map(., content_transformer(removeNumbers)) %>% 
				tm_map(., content_transformer(removePunctuation)) %>%
				tm_map(., content_transformer(stripWhitespace)) %>%
				tm_map(., content_transformer(removeWords), stopwords("SMART")) 
TermMatrix_test = DocumentTermMatrix(corpus_test,
                               control = list(dictionary=Terms(TermMatrix_train)))
TermMatrix_test2 = removeSparseTerms(TermMatrix_test, 0.95)
TermMatrix_test2
DF_test <- data.frame(as.matrix(TermMatrix_test2), stringsAsFactors=FALSE)
TermMatrix_test = DocumentTermMatrix(corpus_test,
                               control = list(dictionary=Terms(TermMatrix_train)))
#outcome vector
train_y = Author_name
test_y = Author_name
train_y = as.data.frame(train_y)
```

The PCA dimension reduction technique is used in order to reduce the data from a large number of features into smaller principal properties. 

```{r echo=FALSE, warning=FALSE}
df_train1<-DF_train[,which(colSums(DF_train) != 0)] 
df_test1<-DF_test[,which(colSums(DF_test) != 0)]
df_train1 = df_train1[,intersect(colnames(df_test1),colnames(df_train1))]
df_test1 = df_test1[,intersect(colnames(df_test1),colnames(df_train1))]
mod_pca = prcomp(df_train1,scale=TRUE)
pred_pca=predict(mod_pca,newdata = df_test1)
plot(mod_pca,type='line') 
plot(mod_pca)
var <- apply(mod_pca$x, 2, var)  
prop <- var / sum(var)
cumsum(prop)
plot(cumsum(mod_pca$sdev^2/sum(mod_pca$sdev^2)))
summary(mod_pca) 
```

The same procedure is repeated for the test data. 

```{r echo=FALSE, warning=FALSE}
train_class = data.frame(mod_pca$x[,1:200])
train_class['author']=Author_name
train_load = mod_pca$rotation[,1:200]
test_class_pre <- scale(df_test1) %*% train_load
test_class <- as.data.frame(test_class_pre)
test_class['author']=labels_test
```


Using supervised learning techniques such as predictive models using Random Forest and KNN, the below results are obtainted. 

```{r}
##Random Forest
library(randomForest)
train_class_forest = train_class 
train_class_forest$author = factor(train_class_forest$author) 
randomFor = randomForest(author ~ . ,
                           data=train_class_forest, importance = TRUE)
predict_random<-predict(randomFor,data=test_class)
random_table<-as.data.frame(table(predict_random,as.factor(test_class$author)))
predicted<-predict_random
actual<-as.factor(test_class$author)
temp<-as.data.frame(cbind(actual,predicted))
temp$flag<-ifelse(temp$actual==temp$predicted,1,0)
sum(temp$flag)
sum(temp$flag)*100/2500
```


```{r echo=FALSE, warning=FALSE}
trainX = subset(train_class, select = -c(author))
testX = subset(test_class,select=-c(author))
train.author=as.factor(train_class$author)
test.author=as.factor(test_class$author)
```


```{r}
comp<-data.frame("Model"=c("Random Forest","KNN"), "Test.accuracy"=c(70.8,19.1))
comp
ggplot(comp,aes(x=Model,y=Test.accuracy))+geom_col()
```

It is clear from the results that the Random Forest technique performs significantly better than the KNN method by around 39%.